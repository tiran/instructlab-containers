# CPU container comes with CPU-only builds of Torch and llama-cpp. The image
# is considerable smaller than a CUDA or ROCm image.
# SCL and UBI 9 python-311 comes with virtual env, compilers, make, git, and more

# c9s Python 3.11 container from Red Hat's Software Collections
# registry.access.redhat.com/ubi9/python-311 works, too.
ARG BASEIMAGE=quay.io/sclorg/python-311-c9s

FROM ${BASEIMAGE} AS runtime
# PyTorch 2.2.1 does not support torch_compile with 3.12
ARG PYTHON=python3.11
ENV PYTHON="${PYTHON}" \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_COMPILE=1 \
    PS1="(app-root) \w\$ " \
    VIRTUAL_ENV="/opt/app-root" \
    PATH="/opt/rocm/bin:$PATH"

COPY --chown=1001:0 containers/sitecustomize.py ${VIRTUAL_ENV}/lib/${PYTHON}/site-packages/
COPY --chown=1001:0 containers/bin/debug-* ${VIRTUAL_ENV}/bin/


FROM runtime AS builder
ARG PKG_CACHE=on
ENV PKG_CACHE=${PKG_CACHE}
# build as root, so caching works
USER 0

# --no-binary llama_cpp_python to force rebuild
# Force AVX off, https://github.com/ggerganov/llama.cpp/issues/5316
COPY requirements.txt /tmp/
RUN --mount=type=cache,sharing=locked,id=pipcache,target=/root/.cache/pip,mode=775 \
    sed 's/\[.*\]//' /tmp/requirements.txt >/tmp/constraints.txt && \
    if [ "${PKG_CACHE}" == "off" ]; then \
        echo "pip cache off"; \
        export PIP_NO_CACHE_DIR=off; \
    else \
        echo "pip cache on"; \
        export PIP_NO_CACHE_DIR=; \
        export PIP_CACHE_DIR=/root/.cache/pip; \
        pip cache remove llama_cpp_python; \
    fi && \
    ${VIRTUAL_ENV}/bin/pip install wheel && \
    ${VIRTUAL_ENV}/bin/pip install -c /tmp/constraints.txt \
        --index-url https://download.pytorch.org/whl/cpu \
        torch && \
    CMAKE_ARGS="" \
        CFLAGS="-mno-avx" \
        FORCE_CMAKE=1 \
        ${VIRTUAL_ENV}/bin/pip install --no-binary llama_cpp_python -c /tmp/constraints.txt llama_cpp_python && \
    ${VIRTUAL_ENV}/bin/pip install -r /tmp/requirements.txt && \
    find ${VIRTUAL_ENV} -name __pycache__ | xargs rm -rf && \
    chown -R 1001:0 ${VIRTUAL_ENV} 

# install instructlab last
COPY . /tmp/instructlab/
RUN ${VIRTUAL_ENV}/bin/pip install --no-deps /tmp/instructlab && \
    find ${VIRTUAL_ENV} -name __pycache__ | xargs rm -rf && \
    chown -R 1001:0 ${VIRTUAL_ENV}


# create final image from base runtime, copy virtual env into final stage
FROM runtime AS final
COPY --from=builder ${VIRTUAL_ENV}/lib/${PYTHON}/site-packages ${VIRTUAL_ENV}/lib/${PYTHON}/site-packages
COPY --from=builder ${VIRTUAL_ENV}/bin ${VIRTUAL_ENV}/bin
# contains ilab config.yaml, .cache/huggingface, and training data
VOLUME ["/opt/app-root/src"]
CMD ["/bin/bash"]
LABEL name="instructlab-cpu" \
      com.github.instructlab.instructlab.target="cpu" \
      summary="PyTorch, llama.cpp, and InstructLab dependencies for CPU" \
      maintainer="Christian Heimes <cheimes@redhat.com>"
USER 0